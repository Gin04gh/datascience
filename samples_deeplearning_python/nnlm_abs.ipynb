{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural Network Language Model (NNLM)\n",
    "* See\n",
    "    * https://arxiv.org/abs/1509.00685\n",
    "    * Sec. 5 in https://www.amazon.co.jp/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AB%E3%82%88%E3%82%8B%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E5%9D%AA%E4%BA%95-%E7%A5%90%E5%A4%AA/dp/4061529242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, C):\n",
    "        \n",
    "        super(NNLM, self).__init__(\n",
    "            xe = L.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            hh = L.Linear(C * embed_size, hidden_size),\n",
    "            hy = L.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "        self.C = C\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x[-self.C:] # 予測中の文字C個\n",
    "        es = self.xe(chainer.Variable(np.array([x[0]], dtype=\"int32\"))) # １番目の文字をベクトル化\n",
    "        \n",
    "        for i in range(1, len(x)): # 残りのC-1個の文字を順番にベクトル化\n",
    "            e = self.xe(chainer.Variable(np.array([x[i]], dtype=\"int32\")))\n",
    "            es = np.hstack((es.data, e.data)) # ベクトル化した予測中の文字を連結していく\n",
    "            \n",
    "        # 以降は普通の順伝播\n",
    "        h = F.tanh(self.hh(chainer.Variable(es)))\n",
    "        y = self.hy(h)\n",
    "        return y\n",
    "    \n",
    "class NNLM4ENC(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, C):\n",
    "        \n",
    "        super(NNLM4ENC, self).__init__(\n",
    "            xe = L.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            hh = L.Linear(C * embed_size, embed_size)\n",
    "        )\n",
    "        self.C = C\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = x[-self.C:] # 予測中の文字C個\n",
    "        es = self.xe(chainer.Variable(np.array([x[0]], dtype=\"int32\"))) # １番目の文字をベクトル化\n",
    "        \n",
    "        for i in range(1, len(x)): # 残りのC-1個の文字を順番にベクトル化\n",
    "            \n",
    "            e = self.xe(chainer.Variable(np.array([x[i]], dtype=\"int32\")))\n",
    "            es = np.hstack((es.data, e.data)) # ベクトル化した予測中の文字を連結していく\n",
    "            \n",
    "        h = F.tanh(self.hh(chainer.Variable(es)))\n",
    "        return h\n",
    "    \n",
    "class ENC(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, C, Q):\n",
    "        \n",
    "        super(ENC, self).__init__(\n",
    "            nnlm4enc = NNLM4ENC(vocab_size=vocab_size, embed_size=embed_size, C=C), # 予測中の文字を埋め込んで、連結したベクトルを返すところまで\n",
    "            xe = L.EmbedID(vocab_size, embed_size, ignore_label=-1), # 入力文の方を読み込む\n",
    "            ey = L.Linear(embed_size, vocab_size)\n",
    "            \n",
    "        )\n",
    "        self.embed_size = embed_size\n",
    "        self.C = C\n",
    "        self.Q = Q\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # 入力側の埋め込みベクトルのリストを計算する\n",
    "        query = x[:-self.C]\n",
    "        query_vs = [] # 埋め込みベクトルのリスト\n",
    "        \n",
    "        for q in query:\n",
    "            q_v = self.xe(chainer.Variable(np.array([q], dtype=\"int32\")))\n",
    "            query_vs.append(q_v.data)\n",
    "            \n",
    "        query_vs = np.array(query_vs, dtype=\"float32\")\n",
    "        # 出力側の埋め込みベクトルの連結ベクトルを計算する\n",
    "        response_v = self.nnlm4enc(x).data # 連結された埋め込みベクトル\n",
    "        # 2つを使ってAttentionを計算する\n",
    "        query_vs = query_vs.reshape(len(query), self.embed_size)\n",
    "        w = F.matmul(query_vs, F.transpose(response_v)) # 内積を計算、これを確率に正規化する\n",
    "        ws = [] # ウエイトを記録する配列\n",
    "        sum_w = 0 # 合計ウエイト\n",
    "        \n",
    "        for w_ in w:\n",
    "            w_ = F.exp(w_) # ソフトマックスで正規化する\n",
    "            ws.append(w_)\n",
    "            sum_w += w_\n",
    "            \n",
    "        prob = [] # 確率を記録する配列\n",
    "        for w_ in ws:\n",
    "            w_ /= sum_w # 確率に変換\n",
    "            prob.append(w_.data)\n",
    "            \n",
    "        prob = np.array(prob, dtype=\"float32\")\n",
    "        # （省略）入力文ベクトルは平均化する\n",
    "        h = F.matmul(F.transpose(prob), query_vs)\n",
    "        y = self.ey(h)\n",
    "        return y\n",
    "    \n",
    "class ABS(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, C, Q):\n",
    "        \n",
    "        super(ABS, self).__init__(\n",
    "            nnlm = NNLM(vocab_size=vocab_size, embed_size=embed_size, hidden_size=hidden_size, C=C),\n",
    "            enc = ENC(vocab_size=vocab_size, embed_size=embed_size, C=C, Q=Q)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, t=None, train=False):\n",
    "        \n",
    "        y = self.nnlm(x) + self.enc(x)\n",
    "        if train:\n",
    "            t = chainer.Variable(np.array(t, dtype='int32'))\n",
    "            loss = F.softmax_cross_entropy(y, t) # 正解単語と予測単語を照らし合わせて損失を計算\n",
    "            return loss # 損失\n",
    "        else:\n",
    "            return np.argmax(y.data) # 予測値（次の文字）\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.zerograds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    [[\"ABCD\"], [\"EFGHI\"]]\n",
    "]\n",
    "train_data_ids = [\n",
    "    [[0,1,2,3], [4,5,6,7,8]]\n",
    "]\n",
    "\n",
    "EPOCH_NUM = 200\n",
    "EMBED_SIZE = 10\n",
    "HIDDEN_SIZE =10\n",
    "BATCH_SIZE = 5\n",
    "N = 5\n",
    "C = 3\n",
    "Q = 2\n",
    "vocab_size = 9 # ABCDEFGHI\n",
    "\n",
    "# x => t\n",
    "# [0,1,2,3]+[] => 4\n",
    "# [0,1,2,3]+[4] => 5\n",
    "# [0,1,2,3]+[4,5] => 6\n",
    "# [0,1,2,3]+[4,5,6] => 7\n",
    "# [0,1,2,3]+[5,6,7] => 8\n",
    "\n",
    "train_x = [\n",
    "    [0,1,2,3,-1,-1,-1],\n",
    "    [0,1,2,3,-1,-1,4],\n",
    "    [0,1,2,3,-1,4,5],\n",
    "    [0,1,2,3,4,5,6],\n",
    "    [0,1,2,3,5,6,7]\n",
    "]\n",
    "train_t = [ # 予測する出力文の次の文字\n",
    "    [4],\n",
    "    [5],\n",
    "    [6],\n",
    "    [7],\n",
    "    [8]\n",
    "]\n",
    "train_x = np.array(train_x, dtype=\"int32\")\n",
    "train_t = np.array(train_t, dtype=\"int32\")\n",
    "\n",
    "model = ABS(vocab_size=vocab_size, embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, C=C, Q=Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t50\ttotal loss:\t5.27961802482605\ttime:\t0:00:01.301282\n",
      "epoch:\t100\ttotal loss:\t3.247352123260498\ttime:\t0:00:01.374284\n",
      "epoch:\t150\ttotal loss:\t1.960340142250061\ttime:\t0:00:01.375270\n",
      "epoch:\t200\ttotal loss:\t1.1147116422653198\ttime:\t0:00:01.375921\n"
     ]
    }
   ],
   "source": [
    "st = datetime.datetime.now()\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    \n",
    "    opt = chainer.optimizers.Adam()\n",
    "    opt.setup(model)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, t in zip(train_x, train_t): # オンライン学習\n",
    "        model.reset()\n",
    "        loss = model(x=x, t=t, train=True)\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        total_loss += loss.data\n",
    "        opt.update()\n",
    "        #opt.zero_grads()\n",
    "        \n",
    "    if (epoch+1)%50 == 0:\n",
    "        ed = datetime.datetime.now()\n",
    "        print(\"epoch:\\t{}\\ttotal loss:\\t{}\\ttime:\\t{}\".format(epoch+1, total_loss, ed-st))\n",
    "        st = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3 -1 -1 -1] 4\n",
      "[ 0  1  2  3 -1 -1  4] 5\n",
      "[ 0  1  2  3 -1  4  5] 6\n",
      "[0 1 2 3 4 5 6] 7\n",
      "[0 1 2 3 5 6 7] 8\n"
     ]
    }
   ],
   "source": [
    "for x in train_x:\n",
    "    y = model(x=x)\n",
    "    print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
