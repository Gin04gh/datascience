{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2018.06.26*\n",
    "\n",
    "* TensorFlow Eager Execution について\n",
    "* 通常モードのTensorFlow\n",
    "* EagerモードのTensorFlow\n",
    "\n",
    "---\n",
    "\n",
    "**TensorFlow Eager Execution について**\n",
    "\n",
    "---\n",
    "\n",
    "* 2018年1月26日（米国時間）, Googleがオープンソース機械学習ライブラリの最新版「TensorFlow 1.5」を公開.\n",
    "    * 「Eager Execution for TensorFlow」\n",
    "        * Eager Execution for TensorFlowは, 「Define by Run」型のプログラミングスタイルを可能にするインタフェースであり, これを有効にすると, PythonからTensorFlow演算を呼び出してすぐに実行できるようになる.\n",
    "            * Define by Run\n",
    "                * 計算グラフ（ニューラルネットの構造）の構築をデータを流しながら行う.\n",
    "            * Define and Run\n",
    "                * 計算グラフを構築してから, そこにデータを流していく.\n",
    "    * 「TensorFlow Lite」\n",
    "        * モバイルや組み込みデバイス向けのTensorFlowの軽量版. 学習済みのTensorFlowモデルを「.tflite」ファイルに変換し, モバイルデバイスを使って低レイテンシで実行できる.\n",
    "    * GPUアクセラレーション対応の強化\n",
    "        * 新たに「CUDA 9」と「cuDNN 7」をサポート.\n",
    "    \n",
    "    \n",
    "* GoogleはEager Execution for TensorFlowのメリットとして, 下記を挙げている.\n",
    "\n",
    "    * 実行時エラーの即時確認と, Pythonツールと統合された迅速なデバッグ\n",
    "    * 使いやすいPython制御フローを利用した動的モデルのサポート\n",
    "    * カスタムおよび高次勾配の強力なサポート\n",
    "    * ほとんどのTensorFlow演算が利用可能\n",
    "    \n",
    "---\n",
    "\n",
    "**通常モードのTensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 4), (112,), (38, 4), (38,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection\n",
    "from tqdm import tqdm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(iris.data, iris.target)\n",
    "\n",
    "train_x.shape, train_y.shape, valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t5\ttrain/loss:\t1.02495\tvalid/loss:\t1.01906\ttrain/accuracy:\t0.33036\tvalid/accuracy:\t0.34211\n",
      "epoch:\t10\ttrain/loss:\t0.91124\tvalid/loss:\t0.89727\ttrain/accuracy:\t0.65179\tvalid/accuracy:\t0.71053\n",
      "epoch:\t15\ttrain/loss:\t0.75381\tvalid/loss:\t0.72561\ttrain/accuracy:\t0.65179\tvalid/accuracy:\t0.71053\n",
      "epoch:\t20\ttrain/loss:\t0.59629\tvalid/loss:\t0.55483\ttrain/accuracy:\t0.65179\tvalid/accuracy:\t0.71053\n",
      "epoch:\t25\ttrain/loss:\t0.48327\tvalid/loss:\t0.44436\ttrain/accuracy:\t0.86607\tvalid/accuracy:\t0.84211\n",
      "epoch:\t30\ttrain/loss:\t0.40163\tvalid/loss:\t0.36836\ttrain/accuracy:\t0.95536\tvalid/accuracy:\t0.92105\n",
      "100%|██████████| 30/30 [00:00<00:00, 42.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# フローの定義\n",
    "\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_size = 20\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_ph = tf.placeholder(tf.int32, [None])\n",
    "y_oh = tf.one_hot(y_ph, depth=output_size, dtype=tf.float32)\n",
    "\n",
    "fc1_w = tf.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "fc1_b = tf.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "fc1 = tf.nn.relu(tf.matmul(x_ph, fc1_w) + fc1_b)\n",
    "\n",
    "fc2_w = tf.Variable(tf.truncated_normal([hidden_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "fc2_b = tf.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)\n",
    "\n",
    "fc3_w = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=0.1), dtype=tf.float32)\n",
    "fc3_b = tf.Variable(tf.constant(0.1, shape=[output_size]), dtype=tf.float32)\n",
    "y_pre = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_oh, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 学習\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "        \n",
    "        perm = np.random.permutation(len(train_x))\n",
    "        \n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            \n",
    "            batch_x = train_x[perm[i:i+batch_size]]\n",
    "            batch_y = train_y[perm[i:i+batch_size]]\n",
    "            train_step.run(feed_dict={x_ph: batch_x, y_ph: batch_y})\n",
    "            \n",
    "        train_loss = cross_entropy.eval(feed_dict={x_ph: train_x, y_ph: train_y})\n",
    "        valid_loss = cross_entropy.eval(feed_dict={x_ph: valid_x, y_ph: valid_y})\n",
    "        train_acc = accuracy.eval(feed_dict={x_ph: train_x, y_ph: train_y})\n",
    "        valid_acc = accuracy.eval(feed_dict={x_ph: valid_x, y_ph: valid_y})\n",
    "        \n",
    "        if (epoch+1)%5 == 0:\n",
    "            tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(epoch+1, train_loss, valid_loss, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EagerモードのTensorFlow**\n",
    "\n",
    "---\n",
    "\n",
    "* Eagerモードを実行するには, 以下のように初期化を行うが, 一度通常モードで実行してしまうとEagerモードを実行できない."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-43aeb5014f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorFlow version: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5445\u001b[0m       raise ValueError(\n\u001b[0;32m-> 5446\u001b[0;31m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5447\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5448\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jupyterで実行する場合には一度カーネルを再起動する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 逆も然り. （以下はさっきのコード）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c5468336757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m   \"\"\"\n\u001b[1;32m   1804\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   1806\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "# フローの定義\n",
    "\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_size = 20\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_ph = tf.placeholder(tf.int32, [None])\n",
    "y_oh = tf.one_hot(y_ph, depth=output_size, dtype=tf.float32)\n",
    "\n",
    "fc1_w = tf.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "fc1_b = tf.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "fc1 = tf.nn.relu(tf.matmul(x_ph, fc1_w) + fc1_b)\n",
    "\n",
    "fc2_w = tf.Variable(tf.truncated_normal([hidden_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "fc2_b = tf.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)\n",
    "\n",
    "fc3_w = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=0.1), dtype=tf.float32)\n",
    "fc3_b = tf.Variable(tf.constant(0.1, shape=[output_size]), dtype=tf.float32)\n",
    "y_pre = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_oh, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 学習\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "        \n",
    "        perm = np.random.permutation(len(train_x))\n",
    "        \n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            \n",
    "            batch_x = train_x[perm[i:i+batch_size]]\n",
    "            batch_y = train_y[perm[i:i+batch_size]]\n",
    "            train_step.run(feed_dict={x_ph: batch_x, y_ph: batch_y})\n",
    "            \n",
    "        train_loss = cross_entropy.eval(feed_dict={x_ph: train_x, y_ph: train_y})\n",
    "        valid_loss = cross_entropy.eval(feed_dict={x_ph: valid_x, y_ph: valid_y})\n",
    "        train_acc = accuracy.eval(feed_dict={x_ph: train_x, y_ph: train_y})\n",
    "        valid_acc = accuracy.eval(feed_dict={x_ph: valid_x, y_ph: valid_y})\n",
    "        \n",
    "        if (epoch+1)%5 == 0:\n",
    "            tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(epoch+1, train_loss, valid_loss, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eagerモードでアイリスの問題をやってみる.\n",
    "* 公式ドキュメントのサンプルコード.\n",
    "    * https://www.tensorflow.org/get_started/eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/data/iris_training.csv\n",
      "8192/2194 [================================================================================================================] - 0s 0us/step\n",
      "Epoch 000: Loss: 1.157, Accuracy: 30.000%\n",
      "Epoch 050: Loss: 0.829, Accuracy: 70.000%\n",
      "Epoch 100: Loss: 0.639, Accuracy: 70.000%\n",
      "Epoch 150: Loss: 0.529, Accuracy: 85.833%\n",
      "Epoch 200: Loss: 0.430, Accuracy: 93.333%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "\n",
    "def parse_csv(line):\n",
    "    example_defaults = [[0.], [0.], [0.], [0.], [0]]  # sets field types\n",
    "    parsed_line = tf.decode_csv(line, example_defaults)\n",
    "    # First 4 fields are features, combine into single tensor\n",
    "    features = tf.reshape(parsed_line[:-1], shape=(4,))\n",
    "    # Last field is the label\n",
    "    label = tf.reshape(parsed_line[-1], shape=())\n",
    "    return features, label\n",
    "\n",
    "train_dataset = tf.data.TextLineDataset(train_dataset_fp)\n",
    "train_dataset = train_dataset.skip(1)             # skip the first header row\n",
    "train_dataset = train_dataset.map(parse_csv)      # parse each row\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000)  # randomize\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(4,)),  # input shape required\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "    epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for x, y in train_dataset:\n",
    "        # Optimize the model\n",
    "        grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss(model, x, y))  # add current batch loss\n",
    "        # compare predicted label to actual label\n",
    "        epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlowの便利？な関数がめっちゃ使われてて分かりづらい.\n",
    "* tf.data はTensorFlowだけの世界で閉じることができるので, 別途使ってみると便利かも？\n",
    "* 上記のコードを参考に, scikit-learnから落としたデータ（numpy）からスタートにして, 少し書き直してみた."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t5\ttrain/loss:\t1.18120\tvalid/loss:\t1.21846\ttrain/accuracy:\t0.26786\tvalid/accuracy:\t0.23684\n",
      "epoch:\t10\ttrain/loss:\t0.96627\tvalid/loss:\t1.00327\ttrain/accuracy:\t0.38393\tvalid/accuracy:\t0.28947\n",
      "epoch:\t15\ttrain/loss:\t0.82718\tvalid/loss:\t0.82987\ttrain/accuracy:\t0.66964\tvalid/accuracy:\t0.71053\n",
      "epoch:\t20\ttrain/loss:\t0.67084\tvalid/loss:\t0.65927\ttrain/accuracy:\t0.75000\tvalid/accuracy:\t0.81579\n",
      "epoch:\t25\ttrain/loss:\t0.56404\tvalid/loss:\t0.53081\ttrain/accuracy:\t0.86607\tvalid/accuracy:\t0.89474\n",
      "epoch:\t30\ttrain/loss:\t0.50282\tvalid/loss:\t0.46308\ttrain/accuracy:\t0.74107\tvalid/accuracy:\t0.81579\n",
      "100%|██████████| 30/30 [00:01<00:00, 16.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x_tf = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
    "train_y_tf = tf.convert_to_tensor(train_y, dtype=tf.int32)\n",
    "valid_x_tf = tf.convert_to_tensor(valid_x, dtype=tf.float32)\n",
    "valid_y_tf = tf.convert_to_tensor(valid_y, dtype=tf.int32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "def lossfun(model, x, y):\n",
    "    y_pre = model(x)\n",
    "    y_oh = tf.one_hot(y, depth=output_size, dtype=tf.float32)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "    return cross_entropy\n",
    "\n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = lossfun(model, x, y)\n",
    "    return tape.gradient(loss, model.variables)\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "    \n",
    "    n, _ = train_x_tf.shape\n",
    "    n = n.value\n",
    "    perm = np.random.permutation(n)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "\n",
    "        batch_x = tf.gather(train_x_tf, perm[i:i+batch_size])\n",
    "        batch_y = tf.gather(train_y_tf, perm[i:i+batch_size])\n",
    "\n",
    "        grads = grad(model, batch_x, batch_y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    train_loss = lossfun(model, train_x_tf, train_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(train_x_tf), axis=1, output_type=tf.int32), train_y_tf)\n",
    "    train_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    valid_loss = lossfun(model, valid_x_tf, valid_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(valid_x_tf), axis=1, output_type=tf.int32), valid_y_tf)\n",
    "    valid_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    if (epoch+1)%5 == 0:\n",
    "        tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(\n",
    "            epoch+1, train_loss, valid_loss, train_acc, valid_acc)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* だいぶ分かってきた.\n",
    "* 高レベルAPIの使い方をしているため, ちょっと生TensorFlowの書き方に寄せてみる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t5\ttrain/loss:\t1.05228\tvalid/loss:\t1.08151\ttrain/accuracy:\t0.35714\tvalid/accuracy:\t0.26316\n",
      "epoch:\t10\ttrain/loss:\t0.99127\tvalid/loss:\t1.02852\ttrain/accuracy:\t0.35714\tvalid/accuracy:\t0.26316\n",
      "epoch:\t15\ttrain/loss:\t0.86419\tvalid/loss:\t0.88195\ttrain/accuracy:\t0.66071\tvalid/accuracy:\t0.68421\n",
      "epoch:\t20\ttrain/loss:\t0.68577\tvalid/loss:\t0.67165\ttrain/accuracy:\t0.66071\tvalid/accuracy:\t0.68421\n",
      "epoch:\t25\ttrain/loss:\t0.53698\tvalid/loss:\t0.49095\ttrain/accuracy:\t0.66071\tvalid/accuracy:\t0.68421\n",
      "epoch:\t30\ttrain/loss:\t0.44538\tvalid/loss:\t0.38509\ttrain/accuracy:\t0.83929\tvalid/accuracy:\t0.84211\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.46it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\"\"\"\n",
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        input_size = 4\n",
    "        output_size = 3\n",
    "        hidden_size = 20\n",
    "        self.fc1_w = tfe.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "        self.fc1_b = tfe.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "        self.fc2_w = tfe.Variable(tf.truncated_normal([hidden_size, hidden_size], stddev=0.1), dtype=tf.float32)\n",
    "        self.fc2_b = tfe.Variable(tf.constant(0.1, shape=[hidden_size]), dtype=tf.float32)\n",
    "        self.fc3_w = tfe.Variable(tf.truncated_normal([hidden_size, output_size], stddev=0.1), dtype=tf.float32)\n",
    "        self.fc3_b = tfe.Variable(tf.constant(0.1, shape=[output_size]), dtype=tf.float32)\n",
    "        self.variables = [\n",
    "            self.fc1_w, self.fc1_b,\n",
    "            self.fc2_w, self.fc2_b,\n",
    "            self.fc3_w, self.fc3_b\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = tf.nn.relu(tf.matmul(x, self.fc1_w) + self.fc1_b)\n",
    "        h = tf.nn.relu(tf.matmul(h, self.fc2_w) + self.fc2_b)\n",
    "        y_pre = tf.matmul(h, self.fc3_w) + self.fc3_b\n",
    "        return y_pre\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "def lossfun(model, x, y):\n",
    "    y_pre = model(x)\n",
    "    y_oh = tf.one_hot(y, depth=output_size, dtype=tf.float32)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "    return cross_entropy\n",
    "\n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = lossfun(model, x, y)\n",
    "    return tape.gradient(loss, model.variables)\n",
    "    \n",
    "train_x_tf = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
    "train_y_tf = tf.convert_to_tensor(train_y, dtype=tf.int32)\n",
    "valid_x_tf = tf.convert_to_tensor(valid_x, dtype=tf.float32)\n",
    "valid_y_tf = tf.convert_to_tensor(valid_y, dtype=tf.int32)\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "    \n",
    "    n, _ = train_x_tf.shape\n",
    "    n = n.value\n",
    "    perm = np.random.permutation(n)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "\n",
    "        batch_x = tf.gather(train_x_tf, perm[i:i+batch_size])\n",
    "        batch_y = tf.gather(train_y_tf, perm[i:i+batch_size])\n",
    "\n",
    "        grads = grad(model, batch_x, batch_y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    train_loss = lossfun(model, train_x_tf, train_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(train_x_tf), axis=1, output_type=tf.int32), train_y_tf)\n",
    "    train_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    valid_loss = lossfun(model, valid_x_tf, valid_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(valid_x_tf), axis=1, output_type=tf.int32), valid_y_tf)\n",
    "    valid_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    if (epoch+1)%5 == 0:\n",
    "        tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(\n",
    "            epoch+1, train_loss, valid_loss, train_acc, valid_acc)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 異なる点としては, 微分計算のところが通常モードとだいぶ様子が違う？\n",
    "    * どうやらGradientTapeクラスで dloss/dw の微分計算をさせるらしい.\n",
    "        * tape = tfe.GradientTape() として, tape(loss, w)とする. \n",
    "        \n",
    "---\n",
    "\n",
    "* kerasじゃない書き方（layers）だと, tfe.Networkクラスが便利そう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t5\ttrain/loss:\t0.97823\tvalid/loss:\t1.01635\ttrain/accuracy:\t0.35714\tvalid/accuracy:\t0.26316\n",
      "epoch:\t10\ttrain/loss:\t0.76805\tvalid/loss:\t0.75222\ttrain/accuracy:\t0.76786\tvalid/accuracy:\t0.81579\n",
      "epoch:\t15\ttrain/loss:\t0.62252\tvalid/loss:\t0.58292\ttrain/accuracy:\t0.89286\tvalid/accuracy:\t0.92105\n",
      "epoch:\t20\ttrain/loss:\t0.52273\tvalid/loss:\t0.46825\ttrain/accuracy:\t0.89286\tvalid/accuracy:\t0.97368\n",
      "epoch:\t25\ttrain/loss:\t0.45645\tvalid/loss:\t0.39486\ttrain/accuracy:\t0.89286\tvalid/accuracy:\t0.97368\n",
      "epoch:\t30\ttrain/loss:\t0.40661\tvalid/loss:\t0.34131\ttrain/accuracy:\t0.92857\tvalid/accuracy:\t0.97368\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.36it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(tfe.Network):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        input_size = 4\n",
    "        output_size = 3\n",
    "        hidden_size = 20\n",
    "        self.fc1 = self.track_layer(tf.layers.Dense(hidden_size, input_shape=(input_size, )))\n",
    "        self.fc2 = self.track_layer(tf.layers.Dense(hidden_size, input_shape=(hidden_size, )))\n",
    "        self.fc3 = self.track_layer(tf.layers.Dense(output_size, input_shape=(hidden_size, )))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        h = tf.nn.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "def lossfun(model, x, y):\n",
    "    y_pre = model(x)\n",
    "    y_oh = tf.one_hot(y, depth=output_size, dtype=tf.float32)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "    return cross_entropy\n",
    "\n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = lossfun(model, x, y)\n",
    "    return tape.gradient(loss, model.variables)\n",
    "    \n",
    "train_x_tf = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
    "train_y_tf = tf.convert_to_tensor(train_y, dtype=tf.int32)\n",
    "valid_x_tf = tf.convert_to_tensor(valid_x, dtype=tf.float32)\n",
    "valid_y_tf = tf.convert_to_tensor(valid_y, dtype=tf.int32)\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "    \n",
    "    n, _ = train_x_tf.shape\n",
    "    n = n.value\n",
    "    perm = np.random.permutation(n)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "\n",
    "        batch_x = tf.gather(train_x_tf, perm[i:i+batch_size])\n",
    "        batch_y = tf.gather(train_y_tf, perm[i:i+batch_size])\n",
    "\n",
    "        grads = grad(model, batch_x, batch_y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    train_loss = lossfun(model, train_x_tf, train_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(train_x_tf), axis=1, output_type=tf.int32), train_y_tf)\n",
    "    train_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    valid_loss = lossfun(model, valid_x_tf, valid_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(valid_x_tf), axis=1, output_type=tf.int32), valid_y_tf)\n",
    "    valid_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    if (epoch+1)%5 == 0:\n",
    "        tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(\n",
    "            epoch+1, train_loss, valid_loss, train_acc, valid_acc)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 高レベルAPIの使い方+Eager だと, やはり高レベルなライブラリで Define by Run の Chainer や PyTorch に似てくる.\n",
    "* 以下のように, Pythonで動的グラフを制御して学習させることが可能に."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t5\ttrain/loss:\t0.93633\tvalid/loss:\t0.94247\ttrain/accuracy:\t0.50000\tvalid/accuracy:\t0.31579\n",
      "epoch:\t10\ttrain/loss:\t0.82767\tvalid/loss:\t0.81756\ttrain/accuracy:\t0.69643\tvalid/accuracy:\t0.71053\n",
      "epoch:\t15\ttrain/loss:\t0.71850\tvalid/loss:\t0.70159\ttrain/accuracy:\t0.72321\tvalid/accuracy:\t0.81579\n",
      "epoch:\t20\ttrain/loss:\t0.46483\tvalid/loss:\t0.40825\ttrain/accuracy:\t0.86607\tvalid/accuracy:\t0.97368\n",
      "epoch:\t25\ttrain/loss:\t0.39936\tvalid/loss:\t0.33973\ttrain/accuracy:\t0.91071\tvalid/accuracy:\t0.97368\n",
      "epoch:\t30\ttrain/loss:\t0.46135\tvalid/loss:\t0.28049\ttrain/accuracy:\t0.90179\tvalid/accuracy:\t0.97368\n",
      "100%|██████████| 30/30 [00:01<00:00, 15.51it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(tfe.Network):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        input_size = 4\n",
    "        output_size = 3\n",
    "        hidden_size = 20\n",
    "        self.fc1 = self.track_layer(tf.layers.Dense(hidden_size, input_shape=(input_size, )))\n",
    "        self.fc2 = self.track_layer(tf.layers.Dense(hidden_size, input_shape=(hidden_size, )))\n",
    "        \n",
    "        self.fc2_2 = self.track_layer(tf.layers.Dense(hidden_size, input_shape=(hidden_size, ))) # もう一つ無駄に順伝播作って\n",
    "        \n",
    "        self.fc3 = self.track_layer(tf.layers.Dense(output_size, input_shape=(hidden_size, )))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        h = tf.nn.relu(self.fc2(h))\n",
    "        \n",
    "        # ランダムにもう一つ無駄に通すという意味のない分岐をするネットワーク\n",
    "        prob = np.random.randn()\n",
    "        if prob > 0:\n",
    "            h = tf.nn.relu(self.fc2_2(h))\n",
    "            \n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "def lossfun(model, x, y):\n",
    "    y_pre = model(x)\n",
    "    y_oh = tf.one_hot(y, depth=output_size, dtype=tf.float32)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_oh, logits=y_pre))\n",
    "    return cross_entropy\n",
    "\n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = lossfun(model, x, y)\n",
    "    return tape.gradient(loss, model.variables)\n",
    "    \n",
    "train_x_tf = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
    "train_y_tf = tf.convert_to_tensor(train_y, dtype=tf.int32)\n",
    "valid_x_tf = tf.convert_to_tensor(valid_x, dtype=tf.float32)\n",
    "valid_y_tf = tf.convert_to_tensor(valid_y, dtype=tf.int32)\n",
    "\n",
    "epoch_num = 30\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "for epoch in tqdm(range(epoch_num), file=sys.stdout):\n",
    "    \n",
    "    n, _ = train_x_tf.shape\n",
    "    n = n.value\n",
    "    perm = np.random.permutation(n)\n",
    "    \n",
    "    for i in range(0, n, batch_size):\n",
    "\n",
    "        batch_x = tf.gather(train_x_tf, perm[i:i+batch_size])\n",
    "        batch_y = tf.gather(train_y_tf, perm[i:i+batch_size])\n",
    "\n",
    "        grads = grad(model, batch_x, batch_y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    train_loss = lossfun(model, train_x_tf, train_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(train_x_tf), axis=1, output_type=tf.int32), train_y_tf)\n",
    "    train_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    valid_loss = lossfun(model, valid_x_tf, valid_y_tf)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(valid_x_tf), axis=1, output_type=tf.int32), valid_y_tf)\n",
    "    valid_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    if (epoch+1)%5 == 0:\n",
    "        tqdm.write('epoch:\\t{}\\ttrain/loss:\\t{:.5f}\\tvalid/loss:\\t{:.5f}\\ttrain/accuracy:\\t{:.5f}\\tvalid/accuracy:\\t{:.5f}'.format(\n",
    "            epoch+1, train_loss, valid_loss, train_acc, valid_acc)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通常モードでGPUを使う場合は明示的に宣言する必要はなかったが, EagerモードでGPUで使う場合は以下のように使い分けて宣言する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU上に値を持たせる場合\n",
    "with tf.device('/gpu:0'):\n",
    "    x = tf.random_normal([10, 10])\n",
    "    \n",
    "# CPU上に値を持たせる場合\n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.random_normal([10, 10])\n",
    "    \n",
    "# 何も指定しない場合はCPU上に持つ\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "# CPUからGPUへ\n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.random_normal([10, 10])\n",
    "x_gpu = x.gpu()\n",
    "\n",
    "# GPUからCPUへ\n",
    "with tf.device('/gpu:0'):\n",
    "    x = tf.random_normal([10, 10])\n",
    "x_cpu = x.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**感想**\n",
    "\n",
    "* Chainer, PyTorchと比べて特に秀でた要素は見当たらなかったので, Chainer, PyTorch, TensorFlow, TensorFlow Eagerどれでも好きなのを使えば良いかなという印象. (TPUが使えるくらい？）\n",
    "* 結構numpyライクに書けるように？これをGPUで行えるようになると考えると, 機械学習に限らず, 一般的な計算でも便利になる場面があるかも？（データを取り込むところからTensorFlowだけで完結させたら, どう異なるか試したい）\n",
    "* 高レベルAPIで使うか, 低レベルAPIで使うか, 先に決めておかないとTensorFlow死ねる."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
